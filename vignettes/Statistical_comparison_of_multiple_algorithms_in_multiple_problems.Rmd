---
title: "scmamp: Statistical Comparison of Multiple Algorithms in Multiple Problems"
author: "Borja Calvo and Guzmán Santafé"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: refs.bib
abstract: Comparing the results obtained by two or more algorithms in a set of problems is a central task in areas such as machine learning or optimization. Drawing conclusions from these comparisons may require the use of statistical tools such as hypothesis testing. There are some interesting papers, such as @demsar2006 or @garcia2010 that cover this topic. In this manuscript we present **scmamp**, an R package aimed at being a tool that simplifies the whole process of analysing the results obtained when comparing algorithms, from loading the data to the production of plots and tables.
vignette: >
  %\VignetteIndexEntry{scmamp: Statistical Comparison of Multiple Algorithms in Multiple Problems}
  %\VignetteEngine{knitr::docco_linear}
  \usepackage[utf8]{inputenc}
---

Comparing the performance of different algorithms is an essential step in many research and practical computational works. When new algorithms are proposed, they have to be compared with the state of the art; similarly, when an algorithm is used for a particular problem, its parameters need to be tuned to obtain the best possible results.

When the differences are very clear (e.g., when an algorithm is the best in all the problems used in the comparison), the direct comparison of the results may be enough. However, most likely, this direct comparison may be missleading and not enough to extract sound conlcusions; in those cases, the statistical assessment of the results is advisable.

The statistical comparison of algorithms in the context of machine learning has been covered in several papers. In particular, the tools implemented in this package are those presented in @demsar2006, @garcia2008 and @garcia2010; A good review covering, among others, the statistical assessment of the results in the context of supervised classification can be found in @santafe2015.

# Existing tools

Some of the methods presented in the referred papers are well known procedures that are included in classical statistics tools. As an example, p-value correction methods such as Holm (@holm1979) or omnibus tests such as Friedman's (@friedman1937) are implemented in R's base package. However, other methods are not so well known and trivial to implement. Worth of highlighting is Bergmann and Hommel's procedure (@bergmann1988) to correct the p-values when all the algorithms are compared pairwise (see @garcia2008, page 2681).

There are some tools that implement some of the methods included in this package. The first one is KEEL, a Java toolbox that includes a module for the statistical assessment of the results obtained in a given experimentation. However, although it can be used independenty from the rest of the toolbox, it is not a general implementation and it offers (from the GUI) a limited combination of methods.

As an alternative to the direct use of KEEL we have STATService 2.0 (@parejo2012), which is a web service that makes use of the java code in KEEL to run the anlyses. In the same line we have STAC (@rodriguez-fdez2015), a python web service that allows running different types of parametric and non-parametric tests using a simple interface.

The goal of **scmamp** is to provide a simple pipeline that allow any researcher to load the complete set of results, analyse them and produce the material needed for publication (tables and plots). Compared with the tools presented above, when the experimentation is not conducted with KEEL (in which case using the same tool for the statistical analysis seems reasonable), importing the data may require some previous formating. In addition, our package offers the posibility of subsetting the results, which can be handy when the problems can be subdivided in different groups (e.g. based on their size). 

Another difference of with the existing implementations is that the functions that perform the analysis accept additiona test and correction functions, increasing their flexibility. 

Finally, worth of mention is that, although KEEL and STATService generate tables to be directly used in publications, they do not generate plots. In our package we have included two functions to represent, graphically, the results of the comparison. Moreover, performing the analysis in R (instead of using Java) allows the user to easily create his/her own plots.

# Brief examples

In this section we will illustrate the use of the package in two simple situations. For a more elaborated discussion on the use of the different functions the reader is refered to the package's vignettes. To access them, just type `browseVignettes("scmamp")`.

The first example of use is the one shown in @garcia2008. Actually, we will use the set of results presented in that paper, which are included in the package in the variable `data.gh.2008`.

```{r,prompt=TRUE}
library(scmamp)
head(data.gh.2008)
```

The goal is analysing all the pairwise comparisons. Therefore, the first hypothesis to test is whether all the algorithms perform equally or, in contrast, some of them have a significantly different behaviour. Then, all the differences are tested for every pair of algorithms and the resulting p-values are corrected. There are different ways to report the results, depending on the post-hoc analysis. A very intuitive tool is Demsar's critical difference plots. However, although easy to interpret, this plots are based on Nementy's test, which is a very conservative one. There are other alternatives that can be used with more powerful methods (such as Bergmann and Hommel's correction). In this example we will use `drawAlgorithmGraph`, a function that creates a graph based on the corrected p-values.

This analysis can be run with the following code:

Differences using Iman and Davenport's test.

```{r,first_analysis, prompt=TRUE}
imanDavenportTest(data.gh.2008)
```


First alternative, Nemenyi's test (Critical Difference Plot).
```{r,first_analysis_2, prompt=TRUE,fig.keep='all', fig.show='hide', fig.path='./images/', fig.width=8, fig.height=4}
plotCD(results.matrix=data.gh.2008, alpha=0.05)
```

Second alternative, Friedman's post-hoc test with Bergmann and Hommel's correction.

```{r,first_analysis_3, prompt=TRUE,fig.keep='all', fig.show='hide', fig.path='./images/', fig.width=4, fig.height=2}
res <- postHocTest(data=data.gh.2008, test="friedman", correct="bergmann", 
                   use.rank=TRUE)

# corrected p-value matrix
c.pval <- res$corrected.pval
c.pval

# LaTeX formated: No signficances higlighted in bold
bold <- c.pval > 0.05
bold[is.na(bold)] <- FALSE
writeTabular(table=c.pval, format="f", bold=bold, hrule=0, vrule=0)

# Graph including the average ranking and links between algorithms whose 
# are not significant differences
drawAlgorithmGraph(pvalue.matrix=res$corrected.pval, mean.value=res$summary)
```

The critical diference plot generated is shown bellow. As can be seen, there are several pairs where no differences have been found.

![](./images/first_analysis_2-1.pdf)

Bellow this line is the graph generated in the second analysis. In this case, only the pairs C4.5 / NaiveBayes and kNN/CN2 are regarded as equal. In other words, this method is able to detect differences where Nemenyi's test is not.

![](./images/first_analysis_3-1.pdf)


For the second example we will use a dataset where the problems can be subdivided into groups. The data we will use is part of the results in @blum2015, which are also included in the package. Depending on how the experimentation is conducted, we can end up with a number of files, each containing part of the results. Moreover, it can happen that the information we need is not only in the file content, but also in its name. The package includes a set of functions whose goal is simplifying this task. As an illustration of their use, instead of using the preloaded data we will load the results from a set of files distributed with the package. For further information about how data can be loaded please see the corresponding vignette.

```{r, prompt=TRUE}
dir <- paste(system.file('loading_tests', package='scmamp'), 
             "experiment_files", sep="/")
list.files(dir)[1:5]
pattern <- 'rgg_size_([0-9]*)_r_(0.[0-9]*)_([a-z,A-Z,1,2]*).out'
var.names <- c('Size', 'Radius', 'Algorithm')
dataset <- readExperimentDir (directory=dir, names=var.names, fname.pattern=pattern, 
                              alg.var.name='Algorithm', value.col='Evaluation',
                              col.names='Evaluation')
head(dataset)
```

In the above code we can see that there are three variables that are extracted from the file names (defining its pattern): `Size`, `Radius` and `Algorithm`. Note that the latter does not appear in the final dataset, as it has been used to generate the different columns in the table.

In this dataset there are 30 problems for each combination of size and radius. The table reflects, for each problem, the results obtained with 8 algorithms. In this case, we want to compare all the algorithms with the reference one, FrogCOL. Moreover, we want to compare them separately for every value of `Radius` when `Size`  is 1000. For the comparison we will use Wilcoxon's test and the p-values will be corrected using Finner's method and, then, a LaTeX table will be generated. In this table the best results will be highlighted in bold font and those without significant differences will be identified with a superscript. 

First, filter the data.
```{r, prompt=TRUE}
sub.dataset <- filterData(data=dataset, condition='Size==1000', 
                          remove.cols='Size')
```

Now, run the comparison.
```{r, prompt=TRUE}
res <- postHocTest(data=sub.dataset, group.by='Radius', test='wilcoxon',
                   correct='finner', control='FrogCOL')
```

Finally, generate the table.
```{r, prompt=TRUE}
tab <- res$summary
best.res <- t(apply(tab, MARGIN=1, 
                    FUN=function(x) {
                      aux <- rep(FALSE, length(x))
                      aux[x == max(x)] <- TRUE
                      return(aux)
                    }))

no.diff <- res$corrected.pval > 0.05
no.diff[is.na(no.diff)] <- FALSE
writeTabular(table=tab, format='f', bold=best.res, mark=no.diff, 
             hrule=0, vrule=1, print.row.names=FALSE, digits=1)
```

# Conclusions

The package has been designed with the goal of simplifying the processing of the results of comparisons of algorithms, performing the loading, analysis and result generation in a few code lines. This document is a brief introduction to the package. For further details on the use of the functions the reader is refered to the documentation and, particularly, to the vignettes of the package.

# References